# Test Models and Dataset

# Importing Libraries
import pandas as pd
import sys
import os
import contextlib
import io
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import tensorflow as tf

original_stdout = sys.stdout 

def analyze_and_predict(file_path):
    original_stdout = sys.stdout
    output_file_path = "./malware_detection_output.txt"

    # Load a new file for testing 
    test_data = pd.read_csv(file_path, sep="|", low_memory=True)

    # Loading the Models
    # Random Forest
    loaded_random_model = joblib.load('./Models/random_forest_model.joblib')
    print("Random Forest model loaded successfully.")

    # Logistic Regression
    loaded_log_model = joblib.load('./Models/logistic_regression_model.joblib')
    print("Logistic Regression model loaded successfully.")

    # Neural Network
    loaded_nn_model = tf.keras.models.load_model('./Models/neural_network_model')
    print("Neural Network model loaded successfully.")

    # Use the trained Random Forest model for prediction
    rf_prediction = loaded_random_model.predict(test_data)

    # Use the trained Logistic Regression model for prediction
    logistic_prediction = loaded_log_model.predict(test_data)

    # Use the trained Neural Network model for prediction
    nn_predictions = loaded_nn_model.predict(test_data)
    nn_predictions = (nn_predictions >= 0.5).astype(int).flatten()

    # Creating a DataFrame to display predictions in a more readable format
    prediction_results = pd.DataFrame({
        'Sample Index': test_data.index,
        'Random Forest Prediction': rf_prediction,
        'Logistic Regression Prediction': logistic_prediction,
        'Neural Network Prediction': nn_predictions
    })

    # Adding a 'Result' column to indicate whether the sample is predicted as Malicious or Legitimate by majority of models
    prediction_results['Result'] = prediction_results.iloc[:, 1:].mean(axis=1)
    prediction_results['Result'] = prediction_results['Result'].apply(lambda x: 'Malicious' if x >= 0.5 else 'Legitimate')
    
            # Analyze the predictions to determine if the file is malicious or legitimate
    predictions = np.vstack((rf_prediction, logistic_prediction, nn_predictions))
    final_predictions = np.sum(predictions, axis=0)

    threshold = 2  # At least two models should predict the sample as malicious for it to be considered malicious
    malicious_indices = np.where(final_predictions >= threshold)[0]
    legitimate_indices = np.where(final_predictions < threshold)[0]
    
    if len(malicious_indices) > 0:
        return malicious_indices
    
    if len(legitimate_indices) > 0:
        return legitimate_indices
